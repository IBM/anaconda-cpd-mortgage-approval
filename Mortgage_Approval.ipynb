{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "HMDA mortgage application data for 2018. Get the dataset from [here](https://ffiec.cfpb.gov/data-publication/snapshot-national-loan-level-dataset/2018)\n",
    "\n",
    "2018 CSV file is being used for the demo. Either download the dataset inside the notebook or upload the dataset to the Watson Studio project data assets. \n",
    "\n",
    "Note: You have to update the dataset path in the notebook accordingly.\n",
    "\n",
    "\n",
    "**Problem**\n",
    "Posed as a binary classification problem to predict whether mortgage is approved or not\n",
    "\n",
    "\n",
    "**Notebook**\n",
    "Notebook attepmts to demonstrate the entire AI governance cycle, from the time policies for using AI models in applications are made, to the development/deployment/monitoring of the model etc.\n",
    "    \n",
    "**Models**\n",
    "Various models are considered and evaluated on muliple trust dimensions - predictive performance, fairness, explainability, adversarial robustness and robustness to dataset shift. Models considered are:\n",
    "1. Logistic Regression - scikit-learn\n",
    "4. Random Forest Classifier - scikit-learn\n",
    "5. Gradient BoostingClassifier - scikit-learn\n",
    "6. Multi-layer Perceptron - Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# visualization, data wrangling, matrix operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "# model training, evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall,TruePositives,TrueNegatives,FalsePositives,FalseNegatives\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "#Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#save models\n",
    "import pickle\n",
    "\n",
    "#Datasets\n",
    "# from aif360.datasets.meps_dataset_longitudinal import MEPSDatasetLongitudinal, merge\n",
    "\n",
    "# fairness\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "#Bias Mitigation Techniques\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "\n",
    "\n",
    "# adversarial robustness\n",
    "from art.attacks.evasion.hop_skip_jump import HopSkipJump\n",
    "from art.classifiers.scikitlearn import ScikitlearnLogisticRegression, ScikitlearnRandomForestClassifier, ScikitlearnDecisionTreeClassifier\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.classifiers.scikitlearn import ScikitlearnGradientBoostingClassifier\n",
    "from art.metrics import empirical_robustness\n",
    "from art.metrics.metrics import get_crafter\n",
    "\n",
    "# explainability\n",
    "import lime \n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from aix360.metrics import faithfulness_metric, monotonicity_metric\n",
    "\n",
    "pd.set_option(\"display.max_columns\",200)\n",
    "pd.set_option(\"display.max_rows\",200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Useful Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "FACTS = {}\n",
    "CURRENT_PHASE = 'Feature Engineering'\n",
    "\n",
    "def add_fact(fact):\n",
    "    global FACTS\n",
    "    global CURRENT_PHASE\n",
    "    curr_phase = FACTS.get(CURRENT_PHASE)\n",
    "    if (curr_phase is None):\n",
    "        curr_phase = []\n",
    "        FACTS[CURRENT_PHASE] = curr_phase\n",
    "    curr_phase.append(fact)\n",
    "    \n",
    "def save_facts(fname):\n",
    "    with open(fname, \"w\") as outfile: \n",
    "        json.dump(FACTS, outfile)\n",
    "        \n",
    "def read_facts(fname):\n",
    "    with open(fname, 'r') as openfile: \n",
    "        # Reading from json file \n",
    "        json_object = json.load(openfile)\n",
    "        return json_object\n",
    "    \n",
    "def print_facts():\n",
    "    s = json.dumps(FACTS, indent=4)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(model,X,y, scaler = None, data='train', model_name=None):\n",
    "    \"\"\"\n",
    "    Evaluates and prints model metrics scores\n",
    "    \"\"\"\n",
    "    Xdata = X.copy()\n",
    "    \n",
    "    if (scaler is not None):\n",
    "        Xdata = scaler.transform(X)\n",
    "\n",
    "    y_true = y.copy()        \n",
    "    y_pred = model.predict(Xdata)\n",
    "    \n",
    "    ###for mlp, predict returns probabilities, not class\n",
    "    if model_name == 'MLP':\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    y_preda = model.predict_proba(Xdata)\n",
    "    y_preda = y_preda[:,1]\n",
    "    y_preda = y_preda.reshape(y_preda.shape[0],1)\n",
    "    \n",
    "    print('Accuracy for ' + data + ' data: ' + str(accuracy_score(y_true, y_pred)))\n",
    "    print('Confusion Matrix for ' + data + ' data: \\n' + str(confusion_matrix(y_true, y_pred)))\n",
    "    print('Precision for ' + data + ' data: ' + str(precision_score(y_true, y_pred)))\n",
    "    print('Recall for ' + data + ' data: ' + str(recall_score(y_true, y_pred)))\n",
    "    print('F1 score ' + data + ' data: ' + str(f1_score(y_true, y_pred)))\n",
    "    print('Balanced accuracy for ' + data + ' data: ' + str(balanced_accuracy_score(y_true, y_pred)))\n",
    "    print('AUC for ' + data + ' data: ' + str(roc_auc_score(y_true, y_preda))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metrics(model,X,y, scaler = None, model_name=None):\n",
    "    \n",
    "    Xdata = X.copy()\n",
    "    \n",
    "    if (scaler is not None):\n",
    "        Xdata = scaler.transform(X)\n",
    "\n",
    "\n",
    "    y_true = y.copy()\n",
    "    y_pred = model.predict(Xdata)\n",
    "\n",
    "    ###for mlp, predict returns probabilities, not class\n",
    "    if model_name == 'MLP':\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "    y_preda = model.predict_proba(Xdata)\n",
    "    y_preda = y_preda[:,1]\n",
    "    y_preda = y_preda.reshape(y_preda.shape[0],1)\n",
    "\n",
    "        \n",
    "    #create a list and append values of the scores\n",
    "    score_temp = []\n",
    "    score_temp.append(accuracy_score(y_true, y_pred))\n",
    "    score_temp.append(precision_score(y_true, y_pred))\n",
    "    score_temp.append(recall_score(y_true, y_pred))\n",
    "    score_temp.append(f1_score(y_true, y_pred))\n",
    "    score_temp.append(balanced_accuracy_score(y_true, y_pred))\n",
    "    score_temp.append(roc_auc_score(y_true, y_preda))\n",
    "        \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    score_temp.append(int(cm[0][0]))\n",
    "    score_temp.append(int(cm[0][1]))\n",
    "    score_temp.append(int(cm[1][0]))\n",
    "    score_temp.append(int(cm[1][1]))\n",
    "\n",
    "    return score_temp\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_metrics(model, X, y, prot_attrs, prot_attrs_names, scaler = None, model_name=None):\n",
    "    data = X.copy()  #X is dataframe, X_scaled is numpy array\n",
    "        \n",
    "    Xdata = X.copy()\n",
    "    \n",
    "    if (scaler is not None):\n",
    "        Xdata = scaler.transform(X)\n",
    "\n",
    "    y_pred = model.predict(Xdata)\n",
    "    \n",
    "    ###for mlp, predict returns probabilities, not class\n",
    "    if model_name == 'MLP':\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    data['target'] = y_pred\n",
    "    \n",
    "    prot_attr = prot_attrs_names[0]\n",
    "        \n",
    "    # create \"BinaryLabelDataset\" and \"BinaryLabelDatasetMetric\" class instances\n",
    "    if prot_attrs is not None:\n",
    "        data_prot_attrs = prot_attrs.copy()\n",
    "        #display(data_prot_attrs)\n",
    "        #display(data)\n",
    "        #mydf=pd.concat([data,data_prot_attrs],axis=1)\n",
    "        #display(mydf[mydf.isnull().any(axis=1)])\n",
    "        #print('not use prot attrs')\n",
    "        df = BinaryLabelDataset(df=pd.concat([data,data_prot_attrs],axis=1), label_names=['target'], protected_attribute_names=[prot_attr])\n",
    "        #mydf = pd.concat([data,data_prot_attrs],axis=1)\n",
    "    else:\n",
    "        #print('using prot attrs')\n",
    "        df = BinaryLabelDataset(df=data, label_names=['target'], protected_attribute_names=[prot_attr])\n",
    "        #mydf = data\n",
    "        \n",
    "    privileged_groups = [{prot_attr: 1.0}]\n",
    "    unprivileged_groups = [{prot_attr: 0.0}]\n",
    "    #print(mydf[(mydf['target'] == 1.0) & (mydf['derived_race_ethnicity_combination'] == 1.0)].shape) \n",
    "    #print(mydf[(mydf['derived_race_ethnicity_combination'] == 1.0)].shape) \n",
    "    #print(mydf[(mydf['target'] == 1.0) & (mydf['derived_race_ethnicity_combination'] == 0.0)].shape)\n",
    "    #print(mydf[(mydf['derived_race_ethnicity_combination'] == 0.0)].shape) \n",
    "    #display(mydf[['target','gender','derived_race_ethnicity_combination']].head(10))\n",
    "    score = BinaryLabelDatasetMetric(df, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "        \n",
    "    # create a list and append values of the scores\n",
    "    score_temp = []\n",
    "    score_temp.append(score.disparate_impact())\n",
    "    score_temp.append(score.statistical_parity_difference())\n",
    "    \n",
    "    return score_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_metrics(model, X, y, sample_size, scaler = None, model_name=None):\n",
    "    \"\"\"\n",
    "    X - dataset to be subsampled\n",
    "    sample_size - size of the subsample, eventually number of adversarial samples\n",
    "    models - models to attack\n",
    "    \"\"\"\n",
    "    st = time()\n",
    "        \n",
    "    # subsample dataset for untargeted attack\n",
    "    Xdata = X.copy()\n",
    "    samples = Xdata.sample(sample_size, random_state=42)\n",
    "    #print(samples.index)\n",
    "    #return 0.0\n",
    "    #use this is X was a ndarray --- samples = X[np.random.choice(X.shape[0], sample_size, replace=False)]\n",
    "    \n",
    "    if (scaler is not None):\n",
    "        samples = scaler.transform(samples)\n",
    "    else:\n",
    "        samples = samples.values\n",
    "  \n",
    "    try:\n",
    "        score = empirical_robustness(model,samples,attack_name='hsj')\n",
    "        tm = round((time()-st)/60,2)\n",
    "        print(\"{} success!!! Time to evaluate {} samples: {} min.\".format(type(model),sample_size, tm))\n",
    "\n",
    "    except:  \n",
    "        import sys\n",
    "        print(sys.exc_info()[0])\n",
    "        print(\"{} failed!!!\".format(type(model)))\n",
    "        score = 'NA'\n",
    "\n",
    "    # create a list and append values of the scores\n",
    "    score_temp = []\n",
    "    score_temp.append(score)\n",
    "\n",
    "    return score_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monotonicity(model, x, coefs, base):\n",
    "\n",
    "    #find predicted class\n",
    "    pred_class = np.argmax(model.predict_proba(x.reshape(1,-1)), axis=1)[0]  #np.argmax(y_pred, axis=1)\n",
    "    #print('mon pred_class' + str(pred_class))\n",
    "    x_copy = base.copy()\n",
    "\n",
    "    #find indexs of coefficients in increasing order of value\n",
    "    ar = np.argsort(coefs)\n",
    "    pred_probs = np.zeros(x.shape[0])\n",
    "    for ind in np.nditer(ar):\n",
    "        x_copy[ind] = x[ind]\n",
    "        x_copy_pr = model.predict_proba(x_copy.reshape(1,-1))\n",
    "        #print('mon x_copy_pr' + str(x_copy_pr))\n",
    "        pred_probs[ind] = x_copy_pr[0][pred_class]\n",
    "\n",
    "    return np.all(np.diff(pred_probs[ar]) >= 0)\n",
    "\n",
    "def faithfulness(model, x, coefs, base):\n",
    "    #find predicted class\n",
    "    pred_class = np.argmax(model.predict_proba(x.reshape(1,-1)), axis=1)[0]\n",
    "\n",
    "    #find indexs of coefficients in decreasing order of value\n",
    "    ar = np.argsort(-coefs)  #argsort returns indexes of values sorted in increasing order; so do it for negated array\n",
    "    pred_probs = np.zeros(x.shape[0])\n",
    "    for ind in np.nditer(ar):\n",
    "        x_copy = x.copy()\n",
    "        x_copy[ind] = base[ind]\n",
    "        x_copy_pr = model.predict_proba(x_copy.reshape(1,-1))\n",
    "        pred_probs[ind] = x_copy_pr[0][pred_class]\n",
    "\n",
    "    return -np.corrcoef(coefs, pred_probs)[0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CHECK THIS CODE AGAINST AIX NOTEBOOK\n",
    "def lime_explainability(explainer, model, X, y, ncases, scaler = None, model_name=None, random_state=42):  \n",
    "    st = time()\n",
    "\n",
    "    Xdata = X.copy()\n",
    "    samples = Xdata.sample(ncases, random_state=random_state)\n",
    "    #print(samples.index)\n",
    "    #return 0.0\n",
    "    \n",
    "    if (scaler is not None):\n",
    "        samples = scaler.transform(samples)\n",
    "    else:\n",
    "        samples = samples.values        \n",
    "\n",
    "    mon = np.zeros(ncases)\n",
    "    fait = np.zeros(ncases)\n",
    "    base = np.zeros(samples.shape[1])\n",
    "    #define num_features, look at top_labels\n",
    "    for i in range(ncases):\n",
    "\n",
    "        y_pred = model.predict(samples[i].reshape(1, -1))\n",
    "    \n",
    "        ###for mlp, predict returns probabilities, not class\n",
    "        if model_name == 'MLP':\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            \n",
    "        predicted_class = y_pred[0] * 1.0\n",
    "        #print('predicted_class')\n",
    "        #print(predicted_class)\n",
    "        exp = explainer.explain_instance(samples[i], model.predict_proba, num_features=65, top_labels=2)\n",
    "        try:\n",
    "            le = exp.local_exp[predicted_class]\n",
    "        except:\n",
    "            print('exception thrown in explainability')\n",
    "            le = exp.local_exp[0]\n",
    "        m = exp.as_map()\n",
    "\n",
    "        x = samples[i]\n",
    "        coefs = np.zeros(x.shape[0])\n",
    "\n",
    "        for v in le:\n",
    "            coefs[v[0]] = v[1]\n",
    "                \n",
    "        '''\n",
    "            pred_class = predicted_class\n",
    "            print(pred_class)\n",
    "\n",
    "            x_copy = base.copy()\n",
    "\n",
    "            #find indexs of coefficients in increasing order of value\n",
    "            ar = np.argsort(coefs)\n",
    "            pred_probs = np.zeros(x.shape[0])\n",
    "            for ind in np.nditer(ar):\n",
    "                x_copy[ind] = x[ind]\n",
    "                x_copy_pr = model.predict_proba(x_copy.reshape(1,-1))\n",
    "                pred_probs[ind] = x_copy_pr[0][int(pred_class)]\n",
    "        '''\n",
    "        \n",
    "        mon[i] = 0 # monotonicity(model, samples[i], coefs, base) #monotonicity_metric\n",
    "        #tm = round((time()-st)/60,2)\n",
    "        #print('Monotonocity time: ', tm)\n",
    "        \n",
    "        #st = time()\n",
    "        fait[i] = faithfulness(model, samples[i], coefs, base) #faithfulness_metric\n",
    "\n",
    "#         print(\"{} % of test records where explanation is monotonic\".format(np.mean(mon)))\n",
    "#         print(\"Faithfulness metric mean: \", np.mean(fait[~np.isnan(fait)]))\n",
    "#         print(\"Faithfulness metric std. dev.:\", np.std(fait[~np.isnan(fait)]))\n",
    "\n",
    "    # create a list and append values of the scores\n",
    "    score_temp = []\n",
    "    score_temp.append(np.mean(mon))\n",
    "    score_temp.append(np.mean(fait[~np.isnan(fait)]))\n",
    "    score_temp.append(np.std(fait[~np.isnan(fait)]))\n",
    "    tm = round((time()-st)/60,2)\n",
    "    print('Faithfulness time: ', tm)\n",
    "    \n",
    "    return score_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "\n",
    "    instance_weights = None\n",
    "\n",
    "    if (dataset_name == 'HMDA-MORTGAGE-APPROVAL-TRAINING-WITHOUT-PROTECTED-ATTRIBUTES'):\n",
    "        '''\n",
    "        'lei', 'derived_msa_md', 'state_code', 'county_code', 'census_tract',\n",
    "       '', 'purchaser_type', '', '',\n",
    "       '', '', 'intro_rate_period',\n",
    "       'other_nonamortizing_features', '', 'total_units',\n",
    "       '', '', '',\n",
    "       'applicant_age_above_62', 'initially_payable_to_institution', '',\n",
    "       'tract_population', 'tract_minority_population_percent',\n",
    "       'ffiec_msa_md_median_family_income', 'tract_to_msa_income_percentage',\n",
    "       'tract_owner_occupied_units', 'tract_one_to_four_family_homes',\n",
    "       'tract_median_age_of_housing_units',\n",
    "       '', '', '',\n",
    "       ''\n",
    "        '''\n",
    "        data = pd.read_csv('2018_hmda_mortgage_approval_processed_TRAIN.csv')\n",
    "        add_fact('Data for building model taken from file: ' + '2018_hmda_mortgage_approval_processed_TRAIN.csv')\n",
    "\n",
    "        cat = ['conforming_loan_limit','preapproval','loan_term',\n",
    "              'aus_1', 'applicant_age', 'applicant_credit_score_type']\n",
    "        num = ['combined_loan_to_value_ratio', 'property_value', 'income', 'modified_debt_to_income_ratio',\n",
    "              'loan_amount','state_code']\n",
    "        prot_attrs_names = ['derived_race_ethnicity_combination', 'gender']\n",
    "        add_fact('Protected features: ' + str(prot_attrs_names))\n",
    "        add_fact('Privileged value for \\'derived_race_ethnicity_combination\\': Non-Hispanic White')\n",
    "        add_fact('Privileged value for \\'gender\\': Male')\n",
    "        add_fact('Protected features NOT being used for model creation')\n",
    "        \n",
    "        add_fact('Features used for model creation: ' + str(set(cat + num) - set(['state_code'])))\n",
    "        \n",
    "        \n",
    "        gender_map = {'Male': 1.0, 'Female':0.0}\n",
    "        data.replace({'gender': gender_map}, inplace=True)\n",
    "\n",
    "        race_map = {'Non-Hispanic White': 1.0, 'Non-Hispanic Black':0.0}\n",
    "        data.replace({'derived_race_ethnicity_combination': race_map}, inplace=True)\n",
    "        \n",
    "        prot_attrs = data[prot_attrs_names].copy()\n",
    "        \n",
    "        dum = pd.get_dummies(data[cat].astype('category'),prefix_sep='=')\n",
    "        X  = pd.concat([dum, data[num]], axis=1)\n",
    "        y = np.array((data['loan_approved'] == True) + .0)\n",
    "\n",
    "\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=42)\n",
    "\n",
    "        NorthEast = ['ME','VT','NH','MA','RI','CT','NY','NJ','PA','MD','DE']\n",
    "        #ne_states_lar_subset = lar_subset[lar_subset['state_code'].isin(NorthEast)]        \n",
    "        shift_logic = X['state_code'].isin(NorthEast)\n",
    "\n",
    "        #holdout for distribution shift\n",
    "        X_shift = X[shift_logic]\n",
    "        y_shift = y[shift_logic]\n",
    "        prot_attrs_shift = prot_attrs[shift_logic]\n",
    "\n",
    "        #the rest; to be splitted randomly 75/15/10\n",
    "        _X = X[~shift_logic]\n",
    "        _y = y[~shift_logic]\n",
    "        _prot_attrs = prot_attrs[~shift_logic]\n",
    "\n",
    "        X_train, X_test, y_train, y_test, prot_attrs_train, prot_attrs_test = train_test_split(_X, _y, _prot_attrs, test_size = .3, random_state=42) #was 0.1\n",
    "        \n",
    "        #remove 'state_code'\n",
    "        X_train.drop(columns=['state_code'], inplace = True)\n",
    "        X_test.drop(columns=['state_code'], inplace = True)\n",
    "        X_shift.drop(columns=['state_code'], inplace = True)\n",
    "        \n",
    "        \n",
    "        add_fact('Dataset for data-shift consists of \\'state_code\\' in :' + str(NorthEast))\n",
    "        add_fact('Remaining \\'state_code\\'s used for train/test datasets')\n",
    "        add_fact('Training dataset size: ' + str(X_train.shape))\n",
    "        add_fact('Test dataset size: ' + str(X_test.shape))\n",
    "        add_fact('Shift dataset size: ' + str(X_shift.shape))\n",
    "        \n",
    "        X_validate = None\n",
    "        y_validate = None\n",
    "        prot_attrs_validate = None\n",
    "        #prot_attr = 'derived_race_ethnicity_combination=Non-Hispanic White'\n",
    "        \n",
    "    elif (dataset_name == 'HMDA-MORTGAGE-APPROVAL-TRAINING-WITH-PROTECTED-ATTRIBUTES'):\n",
    "        data = pd.read_csv('2018_hmda_mortgage_approval_processed_TRAIN.csv')\n",
    "\n",
    "        cat = ['conforming_loan_limit','preapproval','loan_term',\n",
    "              'aus_1', 'applicant_age', 'applicant_credit_score_type']\n",
    "        num = ['combined_loan_to_value_ratio', 'property_value', 'income', 'modified_debt_to_income_ratio',\n",
    "              'loan_amount','state_code', 'derived_race_ethnicity_combination', 'gender']\n",
    "        prot_attrs_names = ['derived_race_ethnicity_combination', 'gender']\n",
    "        \n",
    "        gender_map = {'Male': 1.0, 'Female':0.0}\n",
    "        data.replace({'gender': gender_map}, inplace=True)\n",
    "\n",
    "        race_map = {'Non-Hispanic White': 1.0, 'Non-Hispanic Black':0.0}\n",
    "        data.replace({'derived_race_ethnicity_combination': race_map}, inplace=True)\n",
    "        \n",
    "        \n",
    "        dum = pd.get_dummies(data[cat].astype('category'),prefix_sep='=')\n",
    "        X  = pd.concat([dum, data[num]], axis=1)\n",
    "        y = np.array((data['loan_approved'] == True) + .0)\n",
    "\n",
    "\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=42)\n",
    "\n",
    "        NorthEast = ['ME','VT','NH','MA','RI','CT','NY','NJ','PA','MD','DE']\n",
    "        #ne_states_lar_subset = lar_subset[lar_subset['state_code'].isin(NorthEast)]        \n",
    "        shift_logic = X['state_code'].isin(NorthEast)\n",
    "\n",
    "        #holdout for distribution shift\n",
    "        X_shift = X[shift_logic]\n",
    "        y_shift = y[shift_logic]\n",
    "        prot_attrs_shift = None\n",
    "\n",
    "        #the rest; to be splitted randomly 75/15/10\n",
    "        _X = X[~shift_logic]\n",
    "        _y = y[~shift_logic]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(_X, _y, test_size = .3, random_state=42) #was 0.1\n",
    "        prot_attrs_train = None\n",
    "        prot_attrs_test = None\n",
    "        \n",
    "        #remove 'state_code'\n",
    "        X_train.drop(columns=['state_code'], inplace = True)\n",
    "        X_test.drop(columns=['state_code'], inplace = True)\n",
    "        X_shift.drop(columns=['state_code'], inplace = True)\n",
    "        \n",
    "        X_validate = None\n",
    "        y_validate = None\n",
    "        prot_attrs_validate = None\n",
    "        #prot_attr = 'derived_race_ethnicity_combination=Non-Hispanic White'\n",
    "        \n",
    "    elif (dataset_name == 'HMDA-MORTGAGE-APPROVAL-VALIDATION-WITHOUT-PROTECTED-ATTRIBUTES'):\n",
    "        data = pd.read_csv('2018_hmda_mortgage_approval_processed_VALIDATION.csv')\n",
    "        add_fact('Data for validating model taken from file: ' + '2018_hmda_mortgage_approval_processed_VALIDATION.csv')\n",
    "\n",
    "        cat = ['conforming_loan_limit','preapproval','loan_term',\n",
    "              'aus_1', 'applicant_age', 'applicant_credit_score_type']\n",
    "        num = ['combined_loan_to_value_ratio', 'property_value', 'income', 'modified_debt_to_income_ratio',\n",
    "              'loan_amount'] #,'state_code']\n",
    "\n",
    "        prot_attrs_names = ['derived_race_ethnicity_combination', 'gender']\n",
    "        \n",
    "        add_fact('Protected features: ' + str(prot_attrs_names))\n",
    "        add_fact('Privileged value for \\'derived_race_ethnicity_combination\\': Non-Hispanic White')\n",
    "        add_fact('Privileged value for \\'gender\\': Male')\n",
    "        add_fact('Protected features NOT being used by model')\n",
    "        \n",
    "        add_fact('Features used by model: ' + str(set(cat + num)))\n",
    "        \n",
    "        gender_map = {'Male': 1.0, 'Female':0.0}\n",
    "        data.replace({'gender': gender_map}, inplace=True)\n",
    "\n",
    "        race_map = {'Non-Hispanic White': 1.0, 'Non-Hispanic Black':0.0}\n",
    "        data.replace({'derived_race_ethnicity_combination': race_map}, inplace=True)\n",
    "\n",
    "        prot_attrs = data[prot_attrs_names].copy()\n",
    "\n",
    "        dum = pd.get_dummies(data[cat].astype('category'),prefix_sep='=')\n",
    "        X  = pd.concat([dum, data[num]], axis=1)\n",
    "        y = np.array((data['loan_approved'] == True) + .0)\n",
    "\n",
    "\n",
    "        X_train = None\n",
    "        y_train = None\n",
    "        prot_attrs_train = None\n",
    "        X_test = None\n",
    "        y_test = None\n",
    "        prot_attrs_test = None\n",
    "        X_shift = None\n",
    "        y_shift = None\n",
    "        prot_attrs_shift = None\n",
    "      \n",
    "        \n",
    "        X_validate = X\n",
    "        y_validate = y\n",
    "        prot_attrs_validate = prot_attrs\n",
    "        #prot_attr = 'derived_race_ethnicity_combination=Non-Hispanic White'\n",
    "        add_fact('Validation dataset size: ' + str(X.shape))\n",
    "        \n",
    "    elif (dataset_name == 'HMDA-MORTGAGE-APPROVAL-VALIDATION-WITH-PROTECTED-ATTRIBUTES'):\n",
    "        data = pd.read_csv('2018_hmda_mortgage_approval_processed_VALIDATION.csv')\n",
    "\n",
    "        cat = ['conforming_loan_limit','preapproval','loan_term',\n",
    "              'aus_1', 'applicant_age', 'applicant_credit_score_type']\n",
    "        num = ['combined_loan_to_value_ratio', 'property_value', 'income', 'modified_debt_to_income_ratio',\n",
    "              'loan_amount', 'derived_race_ethnicity_combination', 'gender'] #,'state_code']\n",
    "\n",
    "        prot_attrs_names = ['derived_race_ethnicity_combination', 'gender']\n",
    "        \n",
    "        gender_map = {'Male': 1.0, 'Female':0.0}\n",
    "        data.replace({'gender': gender_map}, inplace=True)\n",
    "\n",
    "        race_map = {'Non-Hispanic White': 1.0, 'Non-Hispanic Black':0.0}\n",
    "        data.replace({'derived_race_ethnicity_combination': race_map}, inplace=True)\n",
    "\n",
    "        dum = pd.get_dummies(data[cat].astype('category'),prefix_sep='=')\n",
    "        X  = pd.concat([dum, data[num]], axis=1)\n",
    "        y = np.array((data['loan_approved'] == True) + .0)\n",
    "\n",
    "\n",
    "        X_train = None\n",
    "        y_train = None\n",
    "        prot_attrs_train = None\n",
    "        X_test = None\n",
    "        y_test = None\n",
    "        prot_attrs_test = None\n",
    "        X_shift = None\n",
    "        y_shift = None\n",
    "        prot_attrs_shift = None\n",
    "      \n",
    "        \n",
    "        X_validate = X\n",
    "        y_validate = y\n",
    "        prot_attrs_validate = None\n",
    "        #prot_attr = 'derived_race_ethnicity_combination=Non-Hispanic White'  \n",
    "    elif (dataset_name == 'HMDA-MORTGAGE-SIMPLE-MODEL'):\n",
    "        data = pd.read_csv('2018_hmda_mortgage_approval_processed_TRAIN.csv')\n",
    "        add_fact('Data for building model taken from file: ' + '2018_hmda_mortgage_approval_processed_TRAIN.csv')\n",
    "\n",
    "        num = ['combined_loan_to_value_ratio', 'modified_debt_to_income_ratio', 'state_code']\n",
    "        prot_attrs_names = ['derived_race_ethnicity_combination', 'gender']\n",
    "        add_fact('Protected features: ' + str(prot_attrs_names))\n",
    "        add_fact('Privileged value for \\'derived_race_ethnicity_combination\\': Non-Hispanic White')\n",
    "        add_fact('Privileged value for \\'gender\\': Male')\n",
    "        add_fact('Protected features NOT being used for model creation')\n",
    "        \n",
    "        add_fact('Features used for model creation: ' + str(set(num) - set(['state_code'])))\n",
    "        \n",
    "        \n",
    "        gender_map = {'Male': 1.0, 'Female':0.0}\n",
    "        data.replace({'gender': gender_map}, inplace=True)\n",
    "\n",
    "        race_map = {'Non-Hispanic White': 1.0, 'Non-Hispanic Black':0.0}\n",
    "        data.replace({'derived_race_ethnicity_combination': race_map}, inplace=True)\n",
    "        \n",
    "        prot_attrs = data[prot_attrs_names].copy()\n",
    "        \n",
    "        X  = data[num]\n",
    "        y = np.array((data['loan_approved'] == True) + .0)\n",
    "\n",
    "\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=42)\n",
    "\n",
    "        NorthEast = ['ME','VT','NH','MA','RI','CT','NY','NJ','PA','MD','DE']\n",
    "        #ne_states_lar_subset = lar_subset[lar_subset['state_code'].isin(NorthEast)]        \n",
    "        shift_logic = X['state_code'].isin(NorthEast)\n",
    "\n",
    "        #holdout for distribution shift\n",
    "        X_shift = X[shift_logic]\n",
    "        y_shift = y[shift_logic]\n",
    "        prot_attrs_shift = prot_attrs[shift_logic]\n",
    "\n",
    "        #the rest; to be splitted randomly 75/15/10\n",
    "        _X = X[~shift_logic]\n",
    "        _y = y[~shift_logic]\n",
    "        _prot_attrs = prot_attrs[~shift_logic]\n",
    "\n",
    "        X_train, X_test, y_train, y_test, prot_attrs_train, prot_attrs_test = train_test_split(_X, _y, _prot_attrs, test_size = .3, random_state=42) #was 0.1\n",
    "        \n",
    "        #remove 'state_code'\n",
    "        X_train.drop(columns=['state_code'], inplace = True)\n",
    "        X_test.drop(columns=['state_code'], inplace = True)\n",
    "        X_shift.drop(columns=['state_code'], inplace = True)\n",
    "        \n",
    "        \n",
    "        add_fact('Dataset for data-shift consists of \\'state_code\\' in :' + str(NorthEast))\n",
    "        add_fact('Remaining \\'state_code\\'s used for train/test datasets')\n",
    "        add_fact('Training dataset size: ' + str(X_train.shape))\n",
    "        add_fact('Test dataset size: ' + str(X_test.shape))\n",
    "        add_fact('Shift dataset size: ' + str(X_shift.shape))\n",
    "        \n",
    "        X_validate = None\n",
    "        y_validate = None\n",
    "        prot_attrs_validate = None\n",
    "        #prot_attr = 'derived_race_ethnicity_combination=Non-Hispanic White'\n",
    "\n",
    "\n",
    "\n",
    "    return X_train, y_train, prot_attrs_train, X_validate, y_validate, prot_attrs_validate, X_test, y_test, prot_attrs_test, X_shift, y_shift, prot_attrs_shift, prot_attrs_names, instance_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, adv_model, model_type, X, y, prot_attrs, prot_attrs_names, explainer, scaler, num_of_adversarial_examples, num_of_cases_to_explain):\n",
    "    accuracy_table = {}\n",
    "    fairness_table = {}\n",
    "    adversarial_table = {}\n",
    "    explainability_table = {}\n",
    "\n",
    "    if X is not None:\n",
    "        accuracy_table[model_type] = accuracy_metrics(model,X,y, scaler, model_type)\n",
    "        \n",
    "        ind=['Accuracy', 'Precision', 'Recall', 'F1', 'Balanced Accuracy', 'AUC', 'TN', 'FP','FN', 'TP']\n",
    "        leng = len(ind)\n",
    "        fact = ''\n",
    "        for ii in range(leng):\n",
    "            if (ii > 0):\n",
    "                fact += ', '\n",
    "            fact += ind[ii]+ ' = ' + str(accuracy_table[model_type][ii])\n",
    "        add_fact('Accuracy metrics for model type ' + model_type + ' :' + fact)\n",
    "\n",
    "\n",
    "        fairness_table[model_type] = fairness_metrics(model,X,y, prot_attrs, prot_attrs_names, scaler, model_type)\n",
    "        ind=['Disparate Impact', 'Statistical Parity Difference']\n",
    "        leng = len(ind)\n",
    "        fact = ''\n",
    "        for ii in range(leng):\n",
    "            if (ii > 0):\n",
    "                fact += ', '\n",
    "            fact += ind[ii]+ ' = ' + str(fairness_table[model_type][ii])\n",
    "        add_fact('Fairness metrics for model type ' + model_type + ' :' + fact)\n",
    "\n",
    "            \n",
    "        adversarial_table[model_type] = adversarial_metrics(adv_model,X,y, num_of_adversarial_examples, scaler, model_type)\n",
    "        ind=['Empirical Robustness']\n",
    "        leng = len(ind)\n",
    "        fact = ''\n",
    "        for ii in range(leng):\n",
    "            if (ii > 0):\n",
    "                fact += ', '\n",
    "            fact += ind[ii]+ ' = ' + str(adversarial_table[model_type][ii])\n",
    "        add_fact('Adversarial robustness metrics for model type ' + model_type + ' :' + fact)\n",
    "\n",
    "\n",
    "        explainability_table[model_type] = lime_explainability(explainer, model, X, y, num_of_cases_to_explain, scaler, model_type)\n",
    "        ind=['Monoticity %', 'Faithfulness mean', 'Faitfulness std']\n",
    "        leng = len(ind)\n",
    "        fact = ''\n",
    "        for ii in range(leng):\n",
    "            if (ii > 0):\n",
    "                fact += ', '\n",
    "            fact += ind[ii]+ ' = ' + str(explainability_table[model_type][ii])\n",
    "        add_fact('Explainability metrics for model type ' + model_type + ' :' + fact)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    accuracy_table_df = pd.DataFrame(accuracy_table, index=['Accuracy', 'Precision', 'Recall', 'F1', 'Balanced Accuracy', 'AUC', 'TN', 'FP','FN', 'TP'])\n",
    "\n",
    "\n",
    "    fairness_table_df = pd.DataFrame(fairness_table, index=['Disparate Impact', 'Statistical Parity Difference'])\n",
    "\n",
    "    adversarial_table_df = pd.DataFrame(adversarial_table, index=['Empirical Robustness'])\n",
    "\n",
    "    explainability_table_df = pd.DataFrame(explainability_table, index=['Monoticity %', 'Faithfulness mean', 'Faitfulness std'])\n",
    "\n",
    "    results_table = pd.concat([accuracy_table_df,fairness_table_df,adversarial_table_df,explainability_table_df])\n",
    "    \n",
    "    return results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(model_type, X, y, scaler=None, sample_weight=None, optimize=False):\n",
    "    \n",
    "    sample_weights = None\n",
    "    if (sample_weight is not None):\n",
    "        sample_weights = sample_weight.copy()\n",
    "        \n",
    "    Xdata = X.copy()\n",
    "    ydata = y.copy()\n",
    "    \n",
    "    if (scaler is not None):\n",
    "        Xdata = scaler.transform(Xdata)\n",
    "\n",
    "        \n",
    "    print(\"\\n\\nModel: \", model_type)\n",
    "\n",
    "    if (model_type == 'LR'):\n",
    "\n",
    "        if optimize:\n",
    "\n",
    "            # Create first pipeline for base without reducing features.\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "            from sklearn.metrics import make_scorer\n",
    "            #pipe = Pipeline([('scaler', StandardScaler()), ('classifier' , LogisticRegression(random_state=42))]) #class_weight='balanced'\n",
    "            pipe = Pipeline([('classifier' , LogisticRegression(random_state=42))])\n",
    "            # Create param grid.\n",
    "\n",
    "            param_grid = [\n",
    "                {'classifier__penalty' : ['l2'],\n",
    "                 'classifier__class_weight' : ['balanced', None],\n",
    "                'classifier__C' : np.append(np.logspace(-4, 4, 20),[1]), #np.logspace(-4, 4, 20),\n",
    "                 'classifier__max_iter' : [1000],\n",
    "                'classifier__solver' : ['lbfgs', 'liblinear','newton-cg','sag', 'saga']}] #'liblinear', \n",
    "\n",
    "            scorers = {\n",
    "                #'precision_score': make_scorer(precision_score),\n",
    "                #'recall_score': make_scorer(recall_score),\n",
    "                'accuracy_score': make_scorer(accuracy_score),\n",
    "                'balanced_score': make_scorer(balanced_accuracy_score),\n",
    "                'auc_score': make_scorer(roc_auc_score)\n",
    "            }\n",
    "\n",
    "            # Create grid search object\n",
    "\n",
    "            skf = StratifiedKFold(n_splits=5)\n",
    "            clf = GridSearchCV(pipe, param_grid = param_grid, cv = skf, verbose=True, n_jobs=-1, scoring=scorers, refit = 'auc_score')\n",
    "\n",
    "            # Fit on data\n",
    "\n",
    "            #return clf.fit(Xdata, ydata, sample_weight=sample_weights)\n",
    "            best_gscv = clf.fit(Xdata, ydata, **{'classifier__sample_weight': sample_weights})\n",
    "            best_lr = best_gscv.best_estimator_.named_steps['classifier']\n",
    "            print(best_lr)\n",
    "            return best_lr\n",
    "        else:\n",
    "\n",
    "            clf_logit = LogisticRegression(C=1, class_weight='balanced', penalty='l2', random_state=42)\n",
    "            #clf_logit = LogisticRegression(random_state = 1, solver = 'lbfgs' ,max_iter = 500)\n",
    "\n",
    "\n",
    "            return clf_logit.fit(Xdata, ydata, sample_weight=sample_weights)\n",
    "\n",
    "    elif (model_type == 'RF'):\n",
    "        clf_rf = RandomForestClassifier(n_jobs=-1, max_depth=8,\n",
    "                                      n_estimators=500, class_weight='balanced', random_state=42)\n",
    "        return clf_rf.fit(Xdata, ydata, sample_weight=sample_weights)\n",
    "    \n",
    "    elif (model_type == 'DT'):\n",
    "        clf_rf = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "        return clf_rf.fit(Xdata, ydata, sample_weight=sample_weights)\n",
    "\n",
    "    elif (model_type == 'GBC'):\n",
    "        if optimize:\n",
    "\n",
    "            # Create first pipeline for base without reducing features.\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "            from sklearn.metrics import make_scorer\n",
    "            pipe = Pipeline([('classifier' , GradientBoostingClassifier(random_state=42, learning_rate=0.15, n_estimators=300,n_iter_no_change=5,validation_fraction=0.2))])\n",
    "            # Create param grid.\n",
    "\n",
    "            param_grid = [\n",
    "                {#'classifier__learning_rate' : [0.15,0.1,0.05,0.01,0.005,0.001],\n",
    "                 #'classifier__n_estimators' : [100,250,500,750,1000,1250,1500]\n",
    "                 'classifier__min_samples_leaf':range(25,200,25), \n",
    "                 'classifier__min_samples_split':range(50,300,50)\n",
    "                 \n",
    "                }] #'liblinear', \n",
    "\n",
    "            scorers = {\n",
    "                #'precision_score': make_scorer(precision_score),\n",
    "                #'recall_score': make_scorer(recall_score),\n",
    "                #'accuracy_score': make_scorer(accuracy_score),\n",
    "                #'balanced_score': make_scorer(balanced_accuracy_score),\n",
    "                'auc_score': make_scorer(roc_auc_score)\n",
    "            }\n",
    "\n",
    "            # Create grid search object\n",
    "\n",
    "            skf = StratifiedKFold(n_splits=5)\n",
    "            clf = GridSearchCV(pipe, param_grid = param_grid, cv = skf, verbose=True, n_jobs=-1, scoring=scorers, refit = 'auc_score')\n",
    "\n",
    "            # Fit on data\n",
    "\n",
    "            #return clf.fit(Xdata, ydata, sample_weight=sample_weights)\n",
    "            best_gscv = clf.fit(Xdata, ydata, **{'classifier__sample_weight': sample_weights})\n",
    "            #print(best_gscv.grid_scores_, best_gscv.best_params_, best_gscv.best_score_)\n",
    "            best_gbc = best_gscv.best_estimator_.named_steps['classifier']\n",
    "            print(best_gbc)\n",
    "            return best_gbc\n",
    "        else:\n",
    "            \n",
    "            clf_gb = GradientBoostingClassifier(n_estimators=300, random_state=42)\n",
    "            print(clf_gb)\n",
    "            return clf_gb.fit(Xdata, ydata, sample_weight=sample_weights)\n",
    "    \n",
    "    elif (model_type == 'MLP'):\n",
    "        \n",
    "        w0 = np.bincount(ydata.ravel().astype(int))[0]/np.bincount(ydata.ravel().astype(int))[1]\n",
    "        neg = np.bincount(ydata.ravel().astype(int))[0]\n",
    "        pos = np.bincount(ydata.ravel().astype(int))[1]\n",
    "\n",
    "        total = neg+pos\n",
    "        initial_bias = np.log([pos/neg])\n",
    "        initial_negative_bias = np.log([neg/pos])\n",
    "\n",
    "        clf_mlp = Sequential()\n",
    "        clf_mlp.add(Dense(10,input_dim= Xdata.shape[1], activation='relu'))\n",
    "        clf_mlp.add(BatchNormalization())\n",
    "        clf_mlp.add(Dense(20, activation='relu')) #input_dim= X_train.shape[1],\n",
    "        clf_mlp.add(BatchNormalization())\n",
    "        clf_mlp.add(Dense(4, activation='relu')) #,input_dim= X_train.shape[1]\n",
    "        clf_mlp.add(BatchNormalization())\n",
    "        #clf_mlp.add(Dense(1, activation='relu'))\n",
    "        #clf_mlp.add(Dense(1, activation='sigmoid', bias_initializer= Constant(initial_bias)))\n",
    "        clf_mlp.add(Dense(2, activation='softmax', bias_initializer= Constant(initial_bias)))\n",
    "\n",
    "        #clf_mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',AUC(), Precision(), Recall(),TruePositives(),TrueNegatives(),FalsePositives(),FalseNegatives()])\n",
    "        clf_mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',AUC(), Precision(), Recall(),TruePositives(),TrueNegatives(),FalsePositives(),FalseNegatives()])\n",
    "\n",
    "        #clf_mlp.summary()\n",
    "\n",
    "        weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "        weight_for_1 = (1 / pos)*(total)/2.0\n",
    "        #print('Computed class weights', weight_for_0, weight_for_1)\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        \n",
    "        #import numpy as np\n",
    "        #from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "        #y_integers = np.argmax(y_art_train, axis=1)\n",
    "        #class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "        #d_class_weights = dict(enumerate(class_weights))\n",
    "        #print('Computed weights for one hot', d_class_weights)\n",
    "\n",
    "        ####SAMPLE WEIGHTS USE\n",
    "        from keras.utils import to_categorical\n",
    "        hist = clf_mlp.fit(Xdata, y=to_categorical(ydata), epochs=130, batch_size=256, class_weight=class_weight, sample_weight=sample_weights, verbose=0) #{0:w0,1:1})\n",
    "        #print('fit compete')\n",
    "        \n",
    "        train_predictions_baseline = clf_mlp.predict(Xdata, batch_size=256)\n",
    "        return clf_mlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border: 1px  black solid !important;\n",
    "  color: black !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(database_list, models_list, num_of_cases_to_explain, num_of_adversarial_examples, scaling, optimize, reweighing):\n",
    "\n",
    "    for dataset_name in database_list:\n",
    "        #load & process data\n",
    "        X_train, y_train, prot_attrs_train, X_validate, y_validate, prot_attrs_validate, X_test, y_test, prot_attrs_test, X_shift, y_shift, prot_attrs_shift, prot_attrs_names, instance_weights = load_data(dataset_name)\n",
    "        add_fact('Sensitive attribute(s): '+ str(prot_attrs_names))\n",
    "        sample_weights = None\n",
    "        if (instance_weights is not None):\n",
    "            sample_weights = instance_weights.copy()\n",
    "            #print(sample_weights[:5])\n",
    "            \n",
    "        if reweighing:\n",
    "            add_fact('Performing bias mitigation using Reweighing')\n",
    "            data = X_train.copy()  #X is dataframe\n",
    "            data_y = y_train.copy()\n",
    "            \n",
    "            prot_attr = prot_attrs_names[0]\n",
    "            data['target'] = data_y\n",
    "            \n",
    "            instance_weights_name = None\n",
    "            if (sample_weights is not None):\n",
    "                instance_weights_name = 'instance_weights'\n",
    "                data[instance_weights_name] = sample_weights\n",
    "\n",
    "            # create \"BinaryLabelDataset\" \n",
    "            if prot_attrs_train is not None:\n",
    "                data_prot_attrs = prot_attrs_train.copy()\n",
    "                df = BinaryLabelDataset(df=pd.concat([data, data_prot_attrs],axis=1), label_names=['target'], protected_attribute_names=[prot_attr], instance_weights_name=instance_weights_name)  #bias mitigation wrt 1st protected attribute only\n",
    "            else:\n",
    "                df = BinaryLabelDataset(df=data, label_names=['target'], protected_attribute_names=[prot_attr], instance_weights_name=instance_weights_name)  #bias mitigation wrt 1st protected attribute only\n",
    "\n",
    "            privileged_groups = [{prot_attr: 1}]\n",
    "            unprivileged_groups = [{prot_attr: 0}]\n",
    "\n",
    "            RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                            privileged_groups=privileged_groups)\n",
    "            RW.fit(df)\n",
    "            X_transf = RW.transform(df)\n",
    "            sample_weights = X_transf.instance_weights\n",
    "\n",
    "\n",
    "        scaler = None\n",
    "        if scaling:\n",
    "            scaler = StandardScaler().fit(X_train)\n",
    "            add_fact('Scaling features using StandardScaler()')\n",
    "\n",
    "        \n",
    "        train_table = {}\n",
    "        validate_table = {}\n",
    "        test_table = {}\n",
    "        shift_table = {}\n",
    "\n",
    "        \n",
    "        learned_models = {}\n",
    "\n",
    "        for model_type in models_list:\n",
    "            model = build_model(model_type, X_train, y_train, scaler, sample_weights, optimize)\n",
    "            learned_models[model_type] = model\n",
    "\n",
    "            if (model_type == 'LR'):\n",
    "                adv_model = ScikitlearnLogisticRegression(model=model)\n",
    "            elif (model_type == 'RF'):\n",
    "                adv_model = ScikitlearnRandomForestClassifier(model=model)\n",
    "            elif (model_type == 'GBC'):\n",
    "                adv_model = ScikitlearnGradientBoostingClassifier(model=model)\n",
    "            elif (model_type == 'MLP'):\n",
    "                adv_model = KerasClassifier(model=model)\n",
    "            elif (model_type == 'DT'):\n",
    "                adv_model = ScikitlearnDecisionTreeClassifier(model=model)\n",
    "            else:\n",
    "                adv_model = None\n",
    "            '''\n",
    "            if (model_type == 'LR'):\n",
    "                feature_importances = pd.DataFrame(abs(model.coef_).T,\n",
    "                                               index = X_train.columns,\n",
    "                                                columns=['importance']).sort_values('importance', ascending=False)\n",
    "                print(feature_importances)\n",
    "            elif ((model_type == 'RF') | (model_type == 'GBC')):\n",
    "                feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                                   index = X_train.columns,\n",
    "                                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "                print(feature_importances)\n",
    "            '''\n",
    "\n",
    "            ## CHECK WITH AIX NOTEBOOK TO SEE IF SCALED DATA NEEDS TO BE PASSED IN\n",
    "            if scaling:\n",
    "                explainer = LimeTabularExplainer(scaler.transform(X_train),feature_names=X_train.columns, \n",
    "                                                 class_names=['0','1'], discretize_continuous=True)\n",
    "            else:\n",
    "                explainer = LimeTabularExplainer(X_train.values,feature_names=X_train.columns, \n",
    "                                                 class_names=['0','1'], discretize_continuous=True)\n",
    "                \n",
    "\n",
    "            #print_result(model,X_train, y_train, scaler, 'train')\n",
    "            #print_result(model,X_validate, y_validate, scaler, 'validate', model_type)\n",
    "\n",
    "            if X_train is not None:\n",
    "                add_fact('Evaluating model ' + model_type + ' on Train data')\n",
    "            train_table[model_type] = evaluate_model(model, adv_model, model_type, X_train, y_train, prot_attrs_train, prot_attrs_names, explainer, scaler, num_of_adversarial_examples, num_of_cases_to_explain)\n",
    "\n",
    "            if X_validate is not None:\n",
    "                add_fact('Evaluating model ' + model_type + ' on Validation data')\n",
    "            validate_table[model_type] = evaluate_model(model, adv_model, model_type, X_validate, y_validate, prot_attrs_validate, prot_attrs_names, explainer, scaler, num_of_adversarial_examples, num_of_cases_to_explain)\n",
    "\n",
    "            if X_test is not None:\n",
    "                add_fact('Evaluating model ' + model_type + ' on Test data')\n",
    "            test_table[model_type] = evaluate_model(model, adv_model, model_type, X_test, y_test, prot_attrs_test, prot_attrs_names, explainer, scaler, num_of_adversarial_examples, num_of_cases_to_explain)\n",
    "\n",
    "            \n",
    "            if X_shift is not None:\n",
    "                add_fact('Evaluating model ' + model_type + ' on Shift data')\n",
    "            shift_table[model_type] = evaluate_model(model, adv_model, model_type, X_shift, y_shift, prot_attrs_shift, prot_attrs_names, explainer, scaler, num_of_adversarial_examples, num_of_cases_to_explain)\n",
    "\n",
    "\n",
    "        tt = pd.concat(list(train_table.values()), axis=1)\n",
    "        vt = pd.concat(list(validate_table.values()), axis=1)\n",
    "        te = pd.concat(list(test_table.values()), axis=1)\n",
    "        st = pd.concat(list(shift_table.values()), axis=1)\n",
    "        results_table = pd.concat([tt, vt, te, st],axis=1,keys=('Train','Validate','Test','Shift'))\n",
    "        filename_qualifier = '_'.join(models_list)\n",
    "        if scaling:\n",
    "            filename_qualifier = filename_qualifier + '_scaling'\n",
    "        if optimize:\n",
    "            filename_qualifier = filename_qualifier + '_optimized'\n",
    "        if reweighing:\n",
    "            filename_qualifier = filename_qualifier + '_reweighing'\n",
    "        print('Database name:', filename_qualifier, '_',dataset_name)\n",
    "        display(results_table)\n",
    "        results_table.to_excel(dataset_name+'_'+filename_qualifier+'_results.xls', index=True)\n",
    "        \n",
    "        return learned_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_HMDA_mortgage_approval_dataset_to_catalog(fname='2018_public_lar_csv.csv'):\n",
    "    #create two files (70/30% split) as training and validation datasets\n",
    "    import pandas as pd\n",
    "    iter_csv = pd.read_csv(fname,low_memory=False, iterator=True, chunksize=1000000)\n",
    "    #df = pd.concat([chunk[chunk['field'] > constant] for chunk in iter_csv])\n",
    "    lar = pd.concat(chunk for chunk in iter_csv)\n",
    "\n",
    "    add_fact(\"Raw data filename: \" + '2018_public_lar_csv.csv')\n",
    "    add_fact('Raw dataset size: ' + str(lar.shape))\n",
    "    \n",
    "    train = lar.sample(frac=0.70, random_state=42)\n",
    "    validation = lar.drop(train.index)\n",
    "    \n",
    "    add_fact('Creating datasets for model building (70%) and validation (30%)')\n",
    "    add_fact('Model building dataset: ' + '2018_public_lar_csv_TRAIN.csv')\n",
    "    add_fact('Model building dataset size: ' + str(train.shape))\n",
    "    add_fact('Model validation dataset: ' + '2018_public_lar_csv_VALIDATION.csv')\n",
    "    add_fact('Model validation dataset size: ' + str(validation.shape))\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    train.to_csv('2018_public_lar_csv_TRAIN.csv', index=False)\n",
    "    validation.to_csv('2018_public_lar_csv_VALIDATION.csv', index=False)\n",
    "    print(\"Training set size: \" + str(train.shape))\n",
    "    print(\"Validation set size: \" + str(validation.shape))\n",
    "    print('------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hmda_mortgage_approval_dataset(fname):\n",
    "    import pandas as pd\n",
    "    iter_csv = pd.read_csv(fname,low_memory=False, iterator=True, chunksize=1000000)\n",
    "    #df = pd.concat([chunk[chunk['field'] > constant] for chunk in iter_csv])\n",
    "    lar = pd.concat(chunk for chunk in iter_csv)\n",
    "\n",
    "    add_fact('Dataset name: ' + fname)\n",
    "    add_fact('Dataset size: ' + str(lar.shape))\n",
    "\n",
    "\n",
    "    lar_subset = lar.loc[((lar['loan_purpose'] == 1) &\n",
    "                          (lar['derived_loan_product_type'] == \"Conventional:First Lien\") &\n",
    "                          (lar[\"derived_dwelling_category\"] == \"Single Family (1-4 Units):Site-Built\") &\n",
    "                          (lar[\"open_end_line_of_credit\"] == 2) &\n",
    "                          (lar[\"business_or_commercial_purpose\"] == 2) &\n",
    "                          (lar[\"occupancy_type\"] == 1) &\n",
    "                          (lar[\"reverse_mortgage\"] == 2) &\n",
    "                          (lar[\"negative_amortization\"] == 2) &\n",
    "                          (lar[\"interest_only_payment\"] == 2) &\n",
    "                          (lar[\"conforming_loan_limit\"] != \"U\") &\n",
    "                          (lar[\"balloon_payment\"] == 2)) , :]\n",
    "\n",
    "    add_fact(\"Limiting rows to condition: \" + 'loan_purpose == 1')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'derived_loan_product_type == \"Conventional:First Lien\"')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'derived_dwelling_category == \"Single Family (1-4 Units):Site-Built\"')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'open_end_line_of_credit == 2')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'business_or_commercial_purpose == 2')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'occupancy_type == 1')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'reverse_mortgage == 2')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'negative_amortization == 2')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'interest_only_payment == 2')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'conforming_loan_limit != \"U\"')\n",
    "    add_fact(\"Limiting rows to condition: \" + 'balloon_payment == 2')\n",
    "\n",
    "\n",
    "    print('Data size after initial filtering: ' + str(lar_subset.shape))\n",
    "\n",
    "    print(\"action_taken distribution\")\n",
    "    display(lar_subset['action_taken'].value_counts(normalize=True))\n",
    "    \n",
    "    lar = None\n",
    "\n",
    "\n",
    "    #action_taken\n",
    "    #1 - Loan originated\n",
    "    #2 - Application approved but not accepted\n",
    "    #3 - Application denied\n",
    "    lar_subset = lar_subset.loc[(lar_subset['action_taken'] == 1) | (lar_subset['action_taken'] == 3),:]\n",
    "\n",
    "    add_fact(\"Limiting rows to condition: \" + 'action_taken == 1 [loan originated] or action_taken == 3 [application denied]')\n",
    "    \n",
    "    print(lar_subset.shape)\n",
    "\n",
    "    display(lar_subset['action_taken'].value_counts(normalize=True))\n",
    "\n",
    "    #Race - White=5, Black = 3, Asian =2\n",
    "    #Ethnicity - Hispanic=1, Non-hispanic=2\n",
    "    display(pd.crosstab(lar_subset['applicant_race_1'],lar_subset['applicant_ethnicity_1'], normalize=False))\n",
    "\n",
    "    display(pd.crosstab(lar_subset['applicant_race_1'],lar_subset['derived_race'], normalize=False))\n",
    "\n",
    "    display(pd.crosstab(lar_subset['applicant_ethnicity_1'],lar_subset['derived_ethnicity'], normalize=False))\n",
    "\n",
    "    def race(row):\n",
    "        if ((row['applicant_ethnicity_1'] == 2) and (row['applicant_race_1'] == 5)):  #non-Hispanic Whites are marked as WHITE; all others as NON-WHITE\n",
    "            return 'Non-Hispanic White'\n",
    "        elif ((row['applicant_ethnicity_1'] == 2) and (row['applicant_race_1'] == 3)):\n",
    "            return 'Non-Hispanic Black'\n",
    "        else:\n",
    "            return 'Others'\n",
    "\n",
    "    lar_subset['derived_race_ethnicity_combination'] = lar_subset.apply(lambda row: race(row), axis=1)\n",
    "\n",
    "    display(lar_subset['derived_race_ethnicity_combination'].value_counts())\n",
    "    \n",
    "    add_fact(\"Created new feature: derived_race_ethnicity_combination\")\n",
    "    add_fact(\"Setting derived_race_ethnicity_combination to 'Non-Hispanic White' if applicant_ethnicity_1 == 2 [non Hispanic] and applicant_race_1 == 5 [White]\")\n",
    "    add_fact(\"Setting derived_race_ethnicity_combination to 'Non-Hispanic Black' if applicant_ethnicity_1 == 2 [non Hispanic] and applicant_race_1 == 3 [Black]\")\n",
    "    add_fact(\"Limiting rows to condition: \" + 'derived_race_ethnicity_combination == \"Non-Hispanic Black\" | derived_race_ethnicity_combination == \"Non-Hispanic White\"')\n",
    "\n",
    "    #restrict population to non hispanic white and not hispanic black only\n",
    "    lar_subset = lar_subset.loc[(lar_subset['derived_race_ethnicity_combination'] == 'Non-Hispanic White') | (lar_subset['derived_race_ethnicity_combination'] == 'Non-Hispanic Black'),:]\n",
    "\n",
    "    def loan_approval(row):\n",
    "        if (row['action_taken'] <= 2):  #non-Hispanic Whites are marked as WHITE; all others as NON-WHITE\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    lar_subset['loan_approved'] = lar_subset.apply(lambda row: loan_approval(row), axis=1)\n",
    "\n",
    "    display(pd.crosstab(lar_subset['derived_race_ethnicity_combination'],lar_subset['loan_approved'], normalize=False))\n",
    "\n",
    "    lar_subset['loan_term'].unique()\n",
    "\n",
    "    lar_subset = lar_subset.loc[((lar_subset[\"loan_term\"] == '180') | (lar_subset[\"loan_term\"] == '360')) , :]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'loan_term == 180 or 360')\n",
    "\n",
    "    print(lar_subset.shape)\n",
    "\n",
    "    display(pd.crosstab(lar_subset['derived_race_ethnicity_combination'],lar_subset['loan_approved'], normalize=False))\n",
    "\n",
    "    # 1 is Male, 2 is Female\n",
    "    display(pd.crosstab(lar_subset['applicant_sex'],lar_subset['loan_approved'], normalize=False))\n",
    "\n",
    "    def gender(row):\n",
    "        if (row['applicant_sex'] == 1):  #non-Hispanic Whites are marked as WHITE; all others as NON-WHITE\n",
    "            return 'Male'\n",
    "        elif (row['applicant_sex'] == 2):\n",
    "            return 'Female'\n",
    "        else:\n",
    "            return 'Others'\n",
    "\n",
    "    lar_subset['gender'] = lar_subset.apply(lambda row: gender(row), axis=1)\n",
    "\n",
    "    #restrict population to male and female only\n",
    "    lar_subset = lar_subset.loc[(lar_subset['gender'] == 'Male') | (lar_subset['gender'] == 'Female'),:]\n",
    "    \n",
    "    add_fact(\"Created new feature: gender\")\n",
    "    add_fact(\"Setting gender to 'Male' if applicant_sex == 1\")\n",
    "    add_fact(\"Setting gender to 'Female' if applicant_sex == 2\")\n",
    "    add_fact(\"Limiting rows to condition: \" + 'gender == \"Male\" | gender == \"Female\"')\n",
    "\n",
    "    display(pd.crosstab(lar_subset['gender'],lar_subset['loan_approved'], normalize=False))\n",
    "\n",
    "    print(lar_subset.shape)\n",
    "\n",
    "    lar_subset['loan_amount'].unique()\n",
    "\n",
    "    lar_subset[lar_subset['loan_amount'].isna()].shape\n",
    "\n",
    "    lar_subset['loan_amount'] = lar_subset['loan_amount'].astype('float64')\n",
    "\n",
    "    lar_subset['loan_amount']\n",
    "\n",
    "    lar_subset['combined_loan_to_value_ratio'].unique()  #string and has nan - convert to real and handle nan\n",
    "\n",
    "    \n",
    "\n",
    "    print(lar_subset.shape)\n",
    "     \n",
    "    # Choose only data points with combined_loan_to_value_ratio != Exempt   \n",
    "    lar_subset = lar_subset.loc[lar_subset['combined_loan_to_value_ratio'] != 'Exempt', :]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'combined_loan_to_value_ratio != \"Exempt\"')\n",
    "\n",
    "    print(lar_subset.shape)\n",
    "\n",
    "    lar_subset['combined_loan_to_value_ratio'] = lar_subset['combined_loan_to_value_ratio'].astype('float64')\n",
    "    add_fact(\"Converted feature combined_loan_to_value_ratio from String to Float\")\n",
    "\n",
    "    lar_subset.loc[lar_subset['combined_loan_to_value_ratio'].isna(),['loan_amount','property_value']]\n",
    "\n",
    "    #remove NA values for loan to value ratio\n",
    "    lar_subset = lar_subset.loc[~lar_subset['combined_loan_to_value_ratio'].isna(),:]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'combined_loan_to_value_ratio is not NA')\n",
    "\n",
    "    print(lar_subset.shape)\n",
    "\n",
    "    lar_subset[lar_subset['property_value'] == 'Exempt'].shape\n",
    "\n",
    "    lar_subset['property_value'] = lar_subset['property_value'].astype('float64')\n",
    "    add_fact(\"Converted feature property_value from String to Float\")\n",
    "\n",
    "    lar_subset[lar_subset['property_value'].isna()].shape\n",
    "\n",
    "    #remove NA values for property_value\n",
    "    lar_subset = lar_subset.loc[~lar_subset['property_value'].isna(),:]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'property_value is not NA')\n",
    "    print(lar_subset.shape)\n",
    "\n",
    "    # lar_subset[lar_subset['income'] == 'Exempt'].shape\n",
    "\n",
    "    lar_subset['income'] = lar_subset['income'].astype('float64')\n",
    "    add_fact(\"Converted feature income from String to Float\")\n",
    "    \n",
    "    lar_subset[lar_subset['income'].isna()].shape\n",
    "\n",
    "    #remove NA values for income\n",
    "    lar_subset = lar_subset.loc[~lar_subset['income'].isna(),:]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'income is not NA')\n",
    "    print(lar_subset.shape)\n",
    "\n",
    "    #temp = lar_subset[lar_subset['interest_rate'].isna()]\n",
    "    #temp['action_taken'].value_counts()\n",
    "    display(pd.crosstab(lar_subset['hoepa_status'],lar_subset['loan_approved'], normalize=False))\n",
    "\n",
    "    #hoepa_status is always 3 if loan is declined. so discard from attributes...probably known after loan decision\n",
    "\n",
    "    temp = lar_subset.copy()\n",
    "\n",
    "    temp['int_rate_is_na'] = temp['interest_rate'].isna()\n",
    "\n",
    "    pd.crosstab(temp['int_rate_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    #interest rate is na if loan is declined. so set after loan decision. so remove from set of attributes\n",
    "\n",
    "    temp['disc_pts_is_na'] = temp['discount_points'].isna()\n",
    "    pd.crosstab(temp['disc_pts_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    #remove discount_points as it is set only if loan approved\n",
    "\n",
    "    temp['rate_spread_is_na'] = temp['rate_spread'].isna()\n",
    "    pd.crosstab(temp['rate_spread_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    #remove rate_spread as set once loan is approved\n",
    "\n",
    "    temp['origination_charges_is_na'] = temp['origination_charges'].isna()\n",
    "    pd.crosstab(temp['origination_charges_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    #remove origination_charges\n",
    "\n",
    "    temp['lender_credits_is_na'] = temp['lender_credits'].isna()\n",
    "    pd.crosstab(temp['lender_credits_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    #remove lender_credits\n",
    "\n",
    "    temp['intro_rate_period_is_na'] = temp['intro_rate_period'].isna()\n",
    "    pd.crosstab(temp['intro_rate_period_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    #keep intro_rate_period\n",
    "\n",
    "    pd.crosstab(lar_subset['applicant_credit_score_type'],lar_subset['loan_approved'], normalize=False)\n",
    "\n",
    "    #make sure applicant credit score type is NOT 1111 (Exempt)\n",
    "\n",
    "    pd.crosstab(lar_subset['aus_1'],lar_subset['loan_approved'], normalize=False)\n",
    "\n",
    "    #remove rows with aus_1 == 1111 (Exempt)\n",
    "    lar_subset = lar_subset.loc[lar_subset['aus_1'] != 1111, :]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'aus_1 != \"1111\"')\n",
    "\n",
    "    lar_subset.shape\n",
    "\n",
    "    lar_subset[lar_subset['debt_to_income_ratio'].isna()].shape\n",
    "\n",
    "    temp['debt_to_income_ratio_is_na'] = temp['debt_to_income_ratio'].isna()\n",
    "    pd.crosstab(temp['debt_to_income_ratio_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    #remove rows with debt_to/-income_ratio of NA\n",
    "    lar_subset = lar_subset.loc[~lar_subset['debt_to_income_ratio'].isna(),:]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'debt_to_income_ratio is not NA')\n",
    "\n",
    "    lar_subset.shape\n",
    "\n",
    "    lar_subset['debt_to_income_ratio'].unique()\n",
    "\n",
    "    # lar_subset[lar_subset['debt_to_income_ratio'] == 'Exempt'].shape\n",
    "\n",
    "    #remove rows where debt_to_income_ratio is Exempt\n",
    "    lar_subset = lar_subset.loc[lar_subset['debt_to_income_ratio'] != 'Exempt', :]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'debt_to_income_ratio != \"Exempt\"')\n",
    "\n",
    "    lar_subset.shape\n",
    "\n",
    "    lar_subset['debt_to_income_ratio'].unique()\n",
    "\n",
    "    def debt_to_income_ratio(row):\n",
    "        if (row['debt_to_income_ratio'] == '20%-<30%'):  #non-Hispanic Whites are marked as WHITE; all others as NON-WHITE\n",
    "            return '25'\n",
    "        elif (row['debt_to_income_ratio'] == '30%-<36%'):\n",
    "            return '33'\n",
    "        elif (row['debt_to_income_ratio'] == '50%-60%'):\n",
    "            return '55'\n",
    "        elif (row['debt_to_income_ratio'] == '<20%'):\n",
    "            return '15'\n",
    "        elif (row['debt_to_income_ratio'] == '>60%'):\n",
    "            return '65'\n",
    "        else:\n",
    "            return row['debt_to_income_ratio']\n",
    "\n",
    "    lar_subset['modified_debt_to_income_ratio'] = lar_subset.apply(lambda row: debt_to_income_ratio(row), axis=1)\n",
    "    lar_subset['modified_debt_to_income_ratio'] = lar_subset['modified_debt_to_income_ratio'].astype('float64')\n",
    "\n",
    "    add_fact(\"Creating new feature: modified_debt_to_income_ratio\")\n",
    "    add_fact(\"Setting modified_debt_to_income_ratio to 25,33,55,15,16 corresponding to debt_to_income_ratio = 20%-30%,30%-<36%,50%-60%,<20%,>60% respectively\")\n",
    "    add_fact(\"Converting modified_debt_to_income_ratio from String to Float\")\n",
    "    lar_subset['modified_debt_to_income_ratio'].unique()\n",
    "\n",
    "    pd.crosstab(lar_subset['debt_to_income_ratio'],lar_subset['modified_debt_to_income_ratio'], normalize=False)\n",
    "\n",
    "    lar_subset['applicant_age'].value_counts(normalize=False)\n",
    "\n",
    "    #remove rows where applicant_age is 8888 or 9999 is Exempt\n",
    "    lar_subset = lar_subset.loc[((lar_subset['applicant_age'] != '8888') & (lar_subset['applicant_age'] != '9999')), :]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'applicant_age != \"8888\" & applicant_age != \"9999\"')\n",
    "    \n",
    "    lar_subset.shape\n",
    "\n",
    "    cols = ['loan_purpose','derived_loan_product_type','derived_dwelling_category',\n",
    "            'open_end_line_of_credit','business_or_commercial_purpose','occupancy_type',\n",
    "            'reverse_mortgage','negative_amortization','interest_only_payment',\n",
    "            'balloon_payment','action_taken','applicant_race_1','applicant_ethnicity_1',\n",
    "            'derived_race','derived_ethnicity','applicant_sex','hoepa_status',\n",
    "            'interest_rate','discount_points','rate_spread','origination_charges',\n",
    "            'lender_credits','activity_year']\n",
    "    lar_subset.drop(columns=cols, inplace=True)\n",
    "\n",
    "    add_fact(\"Dropping columns: \"+str(cols))\n",
    "    lar_subset.shape\n",
    "\n",
    "    cols = ['applicant_race_2','applicant_race_3','applicant_race_4','applicant_race_5',\n",
    "            'applicant_ethnicity_2','applicant_ethnicity_3','applicant_ethnicity_4','applicant_ethnicity_5',\n",
    "            'applicant_race_observed','applicant_ethnicity_observed','applicant_sex_observed',\n",
    "            'aus_2','aus_3','aus_4','aus_5']\n",
    "    lar_subset.drop(columns=cols, inplace=True)\n",
    "    add_fact(\"Dropping columns: \"+str(cols))\n",
    "    \n",
    "    lar_subset.shape\n",
    "\n",
    "    co_applicant_columns = [c for c in lar_subset.columns if c.startswith('co_applicant')]\n",
    "    co_applicant_columns\n",
    "    lar_subset.drop(columns=co_applicant_columns, inplace=True)\n",
    "    add_fact(\"Dropping columns: \"+str(co_applicant_columns))\n",
    "\n",
    "    lar_subset.shape\n",
    "\n",
    "    cols = ['construction_method','denial_reason_1','denial_reason_2',\n",
    "            'denial_reason_3','denial_reason_4','derived_sex','lien_status',\n",
    "            'loan_type']\n",
    "    lar_subset.drop(columns=cols, inplace=True)\n",
    "    add_fact(\"Dropping columns: \"+str(cols))\n",
    "    \n",
    "    lar_subset.shape\n",
    "\n",
    "    cols = ['manufactured_home_secured_property_type','manufactured_home_land_property_interest',\n",
    "            'multifamily_affordable_units','submission_of_application']\n",
    "    lar_subset.drop(columns=cols, inplace=True)\n",
    "    add_fact(\"Dropping columns: \"+str(cols))\n",
    "\n",
    "    lar_subset.shape\n",
    "\n",
    "    temp['total_loan_costs_is_na'] = temp['total_loan_costs'].isna()\n",
    "    pd.crosstab(temp['total_loan_costs_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    temp['total_points_and_fees_is_na'] = temp['total_points_and_fees'].isna()\n",
    "    pd.crosstab(temp['total_points_and_fees_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    #remove both total_points_and_fees and total_loan_costs\n",
    "\n",
    "    lar_subset.drop(columns=['total_loan_costs','total_points_and_fees'], inplace=True)\n",
    "\n",
    "    temp['prepayment_penalty_term_is_na'] = temp['prepayment_penalty_term'].isna()\n",
    "    pd.crosstab(temp['prepayment_penalty_term_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    lar_subset.drop(columns=['prepayment_penalty_term'], inplace=True)\n",
    "\n",
    "    add_fact(\"Dropping columns: \"+str(['total_loan_costs','total_points_and_fees','prepayment_penalty_term']))\n",
    "    lar_subset.shape\n",
    "\n",
    "    temp['intro_rate_period_is_na'] = temp['intro_rate_period'].isna()\n",
    "    pd.crosstab(temp['intro_rate_period_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    temp['initially_payable_to_institution_is_na'] = temp['initially_payable_to_institution'].isna()\n",
    "    pd.crosstab(temp['initially_payable_to_institution_is_na'],temp['loan_approved'], normalize=False)\n",
    "\n",
    "    lar_subset['initially_payable_to_institution'].value_counts()\n",
    "\n",
    "    #remove rows with initially_payable_to_institution == 1111 (Exempt)\n",
    "    lar_subset = lar_subset.loc[lar_subset['initially_payable_to_institution'] != 1111, :]\n",
    "    add_fact(\"Limiting rows to condition: \" + 'initially_payable_to_institution != 1111')\n",
    "\n",
    "    print(lar_subset.shape)\n",
    "\n",
    "    print(lar_subset.columns)\n",
    "    \n",
    "    add_fact(\"Size of processed dataset: \" + str(lar_subset.shape))\n",
    "    \n",
    "    return lar_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data & AI Policy officer specifies the governance policy around data and AI models\n",
    "FACTS = {}\n",
    "CURRENT_PHASE = 'Data and AI Models Policy Specification'\n",
    "add_fact('Datasets must be approved and in data catalog')\n",
    "add_fact('Race, ethnicity, and gender of applicant cannot be used in models used to make mortgage related decisions')\n",
    "add_fact('Model predictive performance metrics must minimally include accuracy, balanced_accuracy and AUC score')\n",
    "add_fact('Models must be checked for bias using Disparate Impact')\n",
    "add_fact('Models must be checked for faithfulness of explanations')\n",
    "add_fact('Models must be checked for robustness to Adversarial attacks using using Empirical Robustness metric')\n",
    "add_fact('Models must be checked for robustness to dataset shift')\n",
    "\n",
    "save_facts('policy.json')\n",
    "print_facts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Business owner requests model for predicting mortgage approval\n",
    "#Data scientist requests data to be added to catalog\n",
    "#Data is added to data catalog by data steward as two files - training to be used for building model by data scientist; \n",
    "#validation to be used subsequently by model validator\n",
    "\n",
    "CURRENT_PHASE = 'Data Addition to Data Catalog'\n",
    "add_fact('Model purpose: Predict mortgage approval')\n",
    "add_HMDA_mortgage_approval_dataset_to_catalog()\n",
    "\n",
    "save_facts('data_catalog.json')\n",
    "print_facts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data scientist aka model builder takes traing data from catalog and starts the model building process by preocessing the data\n",
    "CURRENT_PHASE = 'Model Building - Data Processing'\n",
    "\n",
    "lar_subset = process_hmda_mortgage_approval_dataset('2018_public_lar_csv_TRAIN.csv')\n",
    "lar_subset.to_csv('2018_hmda_mortgage_approval_processed_TRAIN.csv', index=False)\n",
    "\n",
    "save_facts('data_processing.json')\n",
    "print_facts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data scientist creates test/train and shift datasets and builds/evaluates models\n",
    "\n",
    "CURRENT_PHASE = 'Model Building - Evaluation'\n",
    "database_list = ['HMDA-MORTGAGE-APPROVAL-TRAINING-WITHOUT-PROTECTED-ATTRIBUTES'] #['AIF360-GERMAN'] #['AIF360-BANK', 'AIF360-ADULT', MEPS, 'ZINDI', 'BANK',  'ADULT']\n",
    "models_list = ['LR', 'RF', 'GBC', 'MLP']\n",
    "num_of_cases_to_explain = 20\n",
    "num_of_adversarial_examples = 10\n",
    "\n",
    "scaling = True\n",
    "optimize = False\n",
    "\n",
    "reweighing = False\n",
    "\n",
    "learned_models = run_experiments(database_list, models_list, num_of_cases_to_explain, num_of_adversarial_examples, scaling, optimize, reweighing)\n",
    "\n",
    "save_facts('model_evaluation.json')\n",
    "print_facts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data scientist creates test/train and shift datasets and builds/evaluates models with bias mitigation\n",
    "\n",
    "CURRENT_PHASE = 'Model Building - Bias Mitigation'\n",
    "\n",
    "database_list = ['HMDA-MORTGAGE-APPROVAL-TRAINING-WITHOUT-PROTECTED-ATTRIBUTES'] #['AIF360-GERMAN'] #['AIF360-BANK', 'AIF360-ADULT', MEPS, 'ZINDI', 'BANK',  'ADULT']\n",
    "models_list = ['LR', 'RF', 'GBC', 'MLP']\n",
    "num_of_cases_to_explain = 20\n",
    "num_of_adversarial_examples = 10\n",
    "\n",
    "scaling = True\n",
    "optimize = False\n",
    "\n",
    "reweighing = True\n",
    "\n",
    "learned_models = run_experiments(database_list, models_list, num_of_cases_to_explain, num_of_adversarial_examples, scaling, optimize, reweighing)\n",
    "\n",
    "save_facts('bias_mitigation.json')\n",
    "print_facts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validator takes facts from data scientist and processes the validation data the same way\n",
    "\n",
    "CURRENT_PHASE = 'Model Validation - Data Processing'\n",
    "\n",
    "lar_subset = process_hmda_mortgage_approval_dataset('2018_public_lar_csv_VALIDATION.csv')\n",
    "lar_subset.to_csv('2018_hmda_mortgage_approval_processed_VALIDATION.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "save_facts('validation_data_processing.json')\n",
    "print_facts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data scientist selects the GBC model as the final model\n",
    "#the validator tests the learnt model against the validation data.\n",
    "#note: validator needs access to training data for feature scaling purposes\n",
    "\n",
    "CURRENT_PHASE = 'Model Validation - Evaluation'\n",
    "accuracy_table = {}\n",
    "fairness_table = {}\n",
    "adversarial_table = {}\n",
    "explainability_table = {}\n",
    "\n",
    "model_type = 'GBC'\n",
    "model = learned_models[model_type]\n",
    "X_train, y_train, prot_attrs_train, _, _, _, _, _, _,_,_,_, prot_attrs_names, _ = load_data('HMDA-MORTGAGE-APPROVAL-TRAINING-WITHOUT-PROTECTED-ATTRIBUTES')\n",
    "_, _, _, X_validate, y_validate, prot_attrs_validate, _, _, _, _, _, _,_,_ = load_data('HMDA-MORTGAGE-APPROVAL-VALIDATION-WITHOUT-PROTECTED-ATTRIBUTES')\n",
    "#X_train, y_train, prot_attrs_train, X_validate, y_validate, prot_attrs_validate, X_test, y_test, prot_attrs_test, X_shift, y_shift, prot_attrs_shift, prot_attrs_names, instance_weights = load_data(dataset_name)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "adv_model = ScikitlearnGradientBoostingClassifier(model=model)\n",
    "\n",
    "explainer = LimeTabularExplainer(scaler.transform(X_train),feature_names=X_train.columns, \n",
    "                                 class_names=['0','1'], discretize_continuous=True)\n",
    "\n",
    "results_table = evaluate_model(model, adv_model, model_type, X_validate, y_validate, prot_attrs_validate, prot_attrs_names, explainer, scaler, num_of_adversarial_examples, num_of_cases_to_explain)\n",
    "\n",
    "display(results_table)\n",
    "\n",
    "save_facts('validation_evaluation.json')\n",
    "print_facts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the validator also carries out a \"sanity check\" by creating/evaluating a simple decision tree model using only debt to income and\n",
    "#loan to value as features\n",
    "\n",
    "FACTS = read_facts('validation_evaluation.json')\n",
    "\n",
    "CURRENT_PHASE = 'Model Validation - Simple Model Sanity Check'\n",
    "\n",
    "\n",
    "database_list = ['HMDA-MORTGAGE-SIMPLE-MODEL'] \n",
    "models_list = ['DT']\n",
    "num_of_cases_to_explain = 20\n",
    "num_of_adversarial_examples = 10\n",
    "\n",
    "scaling = True\n",
    "optimize = False\n",
    "\n",
    "reweighing = True\n",
    "\n",
    "learned_models = run_experiments(database_list, models_list, num_of_cases_to_explain, num_of_adversarial_examples, scaling, optimize, reweighing)\n",
    "\n",
    "save_facts('sanity_check.json')\n",
    "print_facts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validator sends report to Model Validation executive for review who approves it for deployment\n",
    "#model is embedded into fuller application, tested, and deployed\n",
    "\n",
    "#model kpis checked on regular basis\n",
    "\n",
    "#significant deviation triggers model pull/rebuild by Risk officer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
